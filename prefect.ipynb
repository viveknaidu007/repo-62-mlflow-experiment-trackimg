{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prefect==2.16.2\n",
      "  Downloading prefect-2.16.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiosqlite>=0.17.0 (from prefect==2.16.2)\n",
      "  Using cached aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: alembic<2.0.0,>=1.7.5 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (1.13.1)\n",
      "Collecting apprise<2.0.0,>=1.1.0 (from prefect==2.16.2)\n",
      "  Using cached apprise-1.7.4-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting asyncpg>=0.23 (from prefect==2.16.2)\n",
      "  Using cached asyncpg-0.29.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: click<8.2,>=8.0 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (8.1.7)\n",
      "Requirement already satisfied: cryptography>=36.0.1 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (42.0.5)\n",
      "Collecting dateparser<2.0.0,>=1.1.1 (from prefect==2.16.2)\n",
      "  Using cached dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting docker<7.0,>=4.0 (from prefect==2.16.2)\n",
      "  Using cached docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting graphviz>=0.20.1 (from prefect==2.16.2)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting griffe>=0.20.0 (from prefect==2.16.2)\n",
      "  Using cached griffe-0.42.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.0.0 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (3.1.3)\n",
      "Collecting kubernetes<30.0.0,>=24.2.0 (from prefect==2.16.2)\n",
      "  Using cached kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pytz<2025,>=2021.1 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (2024.1)\n",
      "Collecting readchar<5.0.0,>=4.0.0 (from prefect==2.16.2)\n",
      "  Using cached readchar-4.0.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: sqlalchemy!=1.4.33,<3.0.0,>=1.4.22 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from sqlalchemy[asyncio]!=1.4.33,<3.0.0,>=1.4.22->prefect==2.16.2) (2.0.25)\n",
      "Collecting typer>=0.4.2 (from prefect==2.16.2)\n",
      "  Using cached typer-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from prefect==2.16.2)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting asgi-lifespan<3.0,>=1.0 (from prefect==2.16.2)\n",
      "  Using cached asgi_lifespan-2.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=5.3 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (5.3.2)\n",
      "Requirement already satisfied: cloudpickle<4.0,>=2.0 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (3.0.0)\n",
      "Collecting coolname<3.0.0,>=1.0.4 (from prefect==2.16.2)\n",
      "  Using cached coolname-2.2.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting croniter<3.0.0,>=1.0.12 (from prefect==2.16.2)\n",
      "  Using cached croniter-2.0.3-py2.py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (2024.2.0)\n",
      "Collecting httpcore<2.0.0,>=0.15.0 (from prefect==2.16.2)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting httpx!=0.23.2,>=0.23 (from httpx[http2]!=0.23.2,>=0.23->prefect==2.16.2)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.32 in c:\\users\\poppo\\anaconda3\\envs\\genai\\lib\\site-packages (from prefect==2.16.2) (1.33)\n",
      "Collecting jsonschema<5.0.0,>=3.2.0 (from prefect==2.16.2)\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting orjson<4.0,>=3.7 (from prefect==2.16.2)\n",
      "  Using cached orjson-3.10.0.tar.gz (4.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"prefect==2.16.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prefect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprefect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m task, Flow\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@task\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prefect'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "import prefect\n",
    "from prefect import task, Flow\n",
    "\n",
    "@task\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.dropna()\n",
    "\n",
    "@task\n",
    "def preprocess_data(df):\n",
    "    ratings_dict = {5.0: 1, 4.0: 1, 3.0: 0, 2.0: 0, 1.0: 0}\n",
    "    df['sentiment'] = df['Ratings'].map(ratings_dict)\n",
    "    x = df['Review text']\n",
    "    y = df['sentiment']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=50)\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = text.lower()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = text.split()\n",
    "        cleaned_words = [word for word in words if word not in stop_words]\n",
    "        return ' '.join(cleaned_words)\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    x_train = x_train.apply(clean_text).apply(lemmatize_text)\n",
    "    x_test = x_test.apply(clean_text).apply(lemmatize_text)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "@task\n",
    "def train_model(x_train, y_train):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'tfidf__max_features': [1000, 2000, 3000],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__penalty': ['l1', 'l2']\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=pipeline,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,\n",
    "                               scoring='accuracy',\n",
    "                               return_train_score=True,\n",
    "                               verbose=1\n",
    "                               )\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "@task\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    return test_accuracy\n",
    "\n",
    "with Flow(\"Sentiment Analysis\") as flow:\n",
    "    file_path = \"data.csv\"\n",
    "    df = load_data(file_path)\n",
    "    x_train, x_test, y_train, y_test = preprocess_data(df)\n",
    "    model = train_model(x_train, y_train)\n",
    "    test_accuracy = evaluate_model(model, x_test, y_test)\n",
    "\n",
    "flow.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
